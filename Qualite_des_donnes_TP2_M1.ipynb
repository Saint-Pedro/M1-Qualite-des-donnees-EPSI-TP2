{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP4T9O7tbWyrWsUZjtCZUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saint-Pedro/M1-Qualite-des-donnees-EPSI-TP2/blob/main/Qualite_des_donnes_TP2_M1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZzvhjsDYmb3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- BLOC 1 : Installation et Structure ---\n",
        "!pip install \"great_expectations==0.18.19\" pandas numpy pyarrow fastparquet\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Création structure\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "os.makedirs(\"src\", exist_ok=True)\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "# Logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOC 2 ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "# Configuration logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Chemin du fichier\n",
        "INPUT_FILE = \"data/raw/food_sample.parquet\"\n",
        "\n",
        "# --- 1. Chargement ---\n",
        "try:\n",
        "    df_raw = pd.read_parquet(INPUT_FILE)\n",
        "    logger.info(f\"Dataset chargé : {df_raw.shape[0]} lignes, {df_raw.shape[1]} colonnes\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(\"Fichier non trouvé ! Vérifiez le dossier data/raw/\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Sélection des variables ---\n",
        "# Liste cible idéale\n",
        "cols_target = [\n",
        "    'code', 'product_name', 'brands', 'categories',\n",
        "    'nutriscore_grade', 'nutriscore_score',\n",
        "    'energy-kcal_100g', 'fat_100g', 'saturated-fat_100g',\n",
        "    'sugars_100g', 'proteins_100g', 'salt_100g', 'carbohydrates_100g'\n",
        "]\n",
        "\n",
        "# On ne garde que ce qui existe VRAIMENT dans le fichier\n",
        "existing_cols = [c for c in cols_target if c in df_raw.columns]\n",
        "df_selected = df_raw[existing_cols].copy()\n",
        "\n",
        "# Affichage des infos techniques\n",
        "print(\"--- Infos techniques ---\")\n",
        "df_selected.info()\n",
        "\n",
        "# [cite_start]--- 3. Dictionnaire de données (Correction dynamique) --- [cite: 171]\n",
        "\n",
        "# On crée un \"mapping\" (dictionnaire) des définitions pour éviter l'erreur de longueur\n",
        "definitions_map = {\n",
        "    'code': \"Code barre (ID)\",\n",
        "    'product_name': \"Nom du produit\",\n",
        "    'brands': \"Marque\",\n",
        "    'categories': \"Catégorie\",\n",
        "    'nutriscore_grade': \"Grade Nutri-Score (A-E)\",\n",
        "    'nutriscore_score': \"Score numérique\",\n",
        "    'energy-kcal_100g': \"Calories (kcal)\",\n",
        "    'fat_100g': \"Graisses\",\n",
        "    'saturated-fat_100g': \"Graisses saturées\",\n",
        "    'sugars_100g': \"Sucres\",\n",
        "    'proteins_100g': \"Protéines\",\n",
        "    'salt_100g': \"Sel\",\n",
        "    'carbohydrates_100g': \"Glucides\"\n",
        "}\n",
        "\n",
        "# On construit le tableau en allant chercher la définition seulement si la colonne existe\n",
        "data_dict = pd.DataFrame({\n",
        "    \"Variable\": existing_cols,\n",
        "    \"Type Python\": [str(df_selected[c].dtype) for c in existing_cols],\n",
        "    \"Exemple\": [df_selected[c].iloc[0] if not df_selected.empty else \"\" for c in existing_cols],\n",
        "    \"Définition\": [definitions_map.get(c, \"Non défini\") for c in existing_cols]\n",
        "})\n",
        "\n",
        "print(\"\\n--- Dictionnaire des données ---\")\n",
        "display(data_dict)"
      ],
      "metadata": {
        "id": "NiXFgZMDZDvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOC 3 : Audit Qualité  ---\n",
        "import great_expectations as gx\n",
        "\n",
        "print(f\"Version GX utilisée : {gx.__version__}\")\n",
        "\n",
        "# 1. Initialisation Contexte\n",
        "context = gx.get_context(mode=\"ephemeral\")\n",
        "\n",
        "# 2. Création Source & Asset (Syntaxe GX 0.18)\n",
        "datasource = context.sources.add_pandas(name=\"off_datasource\")\n",
        "# On utilise df_selected (créé au Bloc 2)\n",
        "asset = datasource.add_dataframe_asset(name=\"raw_data\", dataframe=df_selected)\n",
        "\n",
        "# 3. Suite de règles\n",
        "suite_name = \"audit_raw_data\"\n",
        "context.add_or_update_expectation_suite(expectation_suite_name=suite_name)\n",
        "\n",
        "batch_request = asset.build_batch_request()\n",
        "validator = context.get_validator(batch_request=batch_request, expectation_suite_name=suite_name)\n",
        "\n",
        "# --- Définition des Règles ---\n",
        "\n",
        "# A. Complétude\n",
        "if 'product_name' in df_selected.columns:\n",
        "    validator.expect_column_values_to_not_be_null(column=\"product_name\")\n",
        "\n",
        "if 'brands' in df_selected.columns:\n",
        "    validator.expect_column_values_to_not_be_null(column=\"brands\")\n",
        "\n",
        "# B. Unicité\n",
        "if 'code' in df_selected.columns:\n",
        "    validator.expect_column_values_to_be_unique(column=\"code\")\n",
        "\n",
        "# C. Validité & D. Conformité\n",
        "# On vérifie l'existence des colonnes avant d'ajouter les règles\n",
        "if 'nutriscore_grade' in df_selected.columns:\n",
        "    validator.expect_column_values_to_be_in_set(column=\"nutriscore_grade\", value_set=['a', 'b', 'c', 'd', 'e'])\n",
        "\n",
        "if 'code' in df_selected.columns:\n",
        "    validator.expect_column_values_to_match_regex(column=\"code\", regex=r\"^\\d+$\")\n",
        "\n",
        "# Règles sur les nutriments\n",
        "nutri_cols_check = ['sugars_100g', 'fat_100g']\n",
        "for col in nutri_cols_check:\n",
        "    if col in df_selected.columns:\n",
        "        validator.expect_column_values_to_be_between(column=col, min_value=0, max_value=100)\n",
        "    else:\n",
        "        logger.warning(f\"Règle ignorée : la colonne '{col}' n'existe pas dans le dataset.\")\n",
        "\n",
        "# 4. Exécution\n",
        "validator.save_expectation_suite(discard_failed_expectations=False)\n",
        "checkpoint = context.add_or_update_checkpoint(name=\"raw_checkpoint\", validator=validator)\n",
        "results = checkpoint.run()\n",
        "\n",
        "# 5. Résultats\n",
        "stats = results[\"run_results\"][list(results[\"run_results\"].keys())[0]][\"validation_result\"][\"statistics\"]\n",
        "print(f\"\\n--- RÉSULTAT AUDIT ---\")\n",
        "print(f\"Taux de succès : {stats['success_percent']:.2f}%\")\n",
        "print(f\"Règles validées : {stats['successful_expectations']} / {stats['evaluated_expectations']}\")"
      ],
      "metadata": {
        "id": "wRgxFiKDZppu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOC 4 : Traitement et Nettoyage ---\n",
        "\n",
        "df_clean = df_selected.copy()\n",
        "logger.info(\"Début du nettoyage...\")\n",
        "\n",
        "# 1. Création des colonnes manquantes\n",
        "target_cols = ['energy-kcal_100g', 'fat_100g', 'saturated-fat_100g', 'sugars_100g', 'proteins_100g', 'salt_100g', 'carbohydrates_100g']\n",
        "for col in target_cols:\n",
        "    if col not in df_clean.columns:\n",
        "        df_clean[col] = np.nan # On crée la colonne vide\n",
        "\n",
        "# 2. Dédoublonnage\n",
        "if 'code' in df_clean.columns:\n",
        "    initial_len = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates(subset=['code'], keep='first')\n",
        "    logger.info(f\"Dédoublonnage : {initial_len - len(df_clean)} doublons supprimés.\")\n",
        "\n",
        "# 3. Nettoyage Types Numériques\n",
        "def clean_numeric(val):\n",
        "    if pd.isna(val): return np.nan\n",
        "    if isinstance(val, (int, float)): return float(val)\n",
        "    import re\n",
        "    val_str = str(val).replace(',', '.')\n",
        "    match = re.search(r\"([\\d\\.]+)\", val_str)\n",
        "    return float(match.group(1)) if match else np.nan\n",
        "\n",
        "for col in target_cols:\n",
        "    df_clean[col] = df_clean[col].apply(clean_numeric)\n",
        "\n",
        "# 4. Standardisation Texte\n",
        "if 'nutriscore_grade' in df_clean.columns:\n",
        "    df_clean['nutriscore_grade'] = df_clean['nutriscore_grade'].astype(str).str.lower()\n",
        "\n",
        "# Sauvegarde\n",
        "OUTPUT_FILE = \"data/processed/food_sample_clean.parquet\"\n",
        "df_clean.to_parquet(OUTPUT_FILE)\n",
        "logger.info(f\"Fichier propre sauvegardé : {OUTPUT_FILE}\")\n",
        "display(df_clean.head(3))"
      ],
      "metadata": {
        "id": "dmMk3A-IaLyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOC 5 : Audit sur données propres ---\n",
        "\n",
        "# Nouvelle source pour les données propres\n",
        "datasource_clean = context.sources.add_pandas(name=\"clean_datasource\")\n",
        "asset_clean = datasource_clean.add_dataframe_asset(name=\"clean_data\", dataframe=df_clean)\n",
        "\n",
        "# On reprend la même suite de règles\n",
        "batch_request_clean = asset_clean.build_batch_request()\n",
        "validator_clean = context.get_validator(batch_request=batch_request_clean, expectation_suite_name=suite_name)\n",
        "\n",
        "# On force l'ajout des règles sur les colonnes qui manquaient avant mais existent maintenant\n",
        "validator_clean.expect_column_values_to_be_between(column=\"sugars_100g\", min_value=0, max_value=100)\n",
        "\n",
        "checkpoint_clean = context.add_or_update_checkpoint(name=\"clean_checkpoint\", validator=validator_clean)\n",
        "results_clean = checkpoint_clean.run()\n",
        "\n",
        "stats_clean = results_clean[\"run_results\"][list(results_clean[\"run_results\"].keys())[0]][\"validation_result\"][\"statistics\"]\n",
        "\n",
        "print(f\"--- COMPARATIF ---\")\n",
        "print(f\"Après nettoyage : {stats_clean['success_percent']:.2f}% de succès\")"
      ],
      "metadata": {
        "id": "HKsenrm9ac9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOC 6 : Analyse Métier ---\n",
        "\n",
        "# Règle 1 : Energie aberrante (> 1000 kcal)\n",
        "# On remplit les NaN par 0 pour pouvoir faire le calcul de filtre sans erreur\n",
        "mask_aberrant_energy = df_clean['energy-kcal_100g'].fillna(0) > 1000\n",
        "\n",
        "# Règle 2 : Somme > 100g\n",
        "df_clean['sum_macro'] = (df_clean['fat_100g'].fillna(0) +\n",
        "                         df_clean['carbohydrates_100g'].fillna(0) +\n",
        "                         df_clean['proteins_100g'].fillna(0) +\n",
        "                         df_clean['salt_100g'].fillna(0))\n",
        "mask_aberrant_sum = df_clean['sum_macro'] > 110\n",
        "\n",
        "print(f\"Produits > 1000 kcal : {mask_aberrant_energy.sum()}\")\n",
        "print(f\"Produits somme > 110g : {mask_aberrant_sum.sum()}\")\n",
        "\n",
        "# Exclusion\n",
        "df_final = df_clean[~mask_aberrant_energy & ~mask_aberrant_sum].copy()\n",
        "print(f\"Taille finale du dataset : {len(df_final)}\")"
      ],
      "metadata": {
        "id": "iTz7FIwnav5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\"\"\n",
        "### MONITORING RECOMMANDÉ\n",
        "1. Taux de complétude 'nutriscore_grade' (Alerte si < 90%)\n",
        "2. Taux de validité Kcal (Alerte si > 1% de valeurs > 900)\n",
        "3. Unicité Code Barre (Bloquant si doublon)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Ye3426Cza4wX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}